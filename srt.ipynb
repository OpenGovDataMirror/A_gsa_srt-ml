{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker session and role\n",
    "\n",
    "Here my make use of the execution role we gave the sagemaker notebook instance. This role provides access to the S3 bucket we created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "s3_bucket = 'srt-sm'\n",
    "\n",
    "#prefix = 'Scikit-LinearLearner-pipeline-srt' #use this for a smaller subset of the training data\n",
    "prefix = 'Sklearn-RandomizedGridSearch' #this is all 993 labeled samples\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Scikit estimator \n",
    "Here we create an instance of a `sagemaker.sklearn.estimator.sklearn` estimator. The constructor accepts several constructor arguments:\n",
    "\n",
    " - **source_dir**: Path (absolute or relative) to the directory with our custom model training source code.\n",
    " - **entry_point**: The path to the Python script SageMaker runs for training and prediction within `source_dir`.\n",
    " - **role**: Role ARN, which is provided by `get_execution_role()`\n",
    " - **train_instance_type (optional)**: The type of SageMaker instances for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types. Also, note that you may need to request an EC2 quota increase for these ml ec2 instance types.\n",
    " - **sagemaker_session (optional)**: The session used to train on Sagemaker, as returned by `sagemaker.Session()`.\n",
    " - **output_path (optional)**: s3 location where you want the training result (model artifacts and optional output files) saved. If not specified, results are stored to a default bucket. If the bucket with the specific name does not exist, the estimator creates the bucket during execution of the `fit()` method.\n",
    " - **train_max_run (optional)**: Timeout in seconds for training (default: 24 * 60 * 60). After this amount of time Amazon SageMaker terminates the job regardless of its current status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "entry_point = 'sklearn_featureizer.py'\n",
    "source_dir = 'pipeline'\n",
    "\n",
    "s3_output_key_prefix = \"training_output\"\n",
    "model_output_path = 's3://{}/{}/{}/{}'.format(s3_bucket, prefix, s3_output_key_prefix, 'model')\n",
    "\n",
    "# terminate model training after 48 hours\n",
    "train_max_run = 48 * 60 * 60\n",
    "\n",
    "grid_search = SKLearn(source_dir=source_dir,\n",
    "                      entry_point=entry_point,\n",
    "                      role=role,\n",
    "                      train_instance_type=\"ml.c5.4xlarge\",\n",
    "                      sagemaker_session=sagemaker_session,\n",
    "                      output_path=model_output_path,\n",
    "                      train_max_run=train_max_run)\n",
    "\n",
    "train_input = f's3://{s3_bucket}/{prefix}/srt_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this will take awhile.\n",
    "grid_search.fit({'train': train_input}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a fitted model (i.e. the best estimator from the Grid Search) in our s3 bucket. We can now deploy this model behind a single endpoint. When this is done, you'll be able to see this endpoint under **Endpoints** in the SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "\n",
    "model = grid_search.create_model(role=role)\n",
    "\n",
    "model.deploy(initial_instance_count=1, \n",
    "             instance_type='ml.c5.xlarge',\n",
    "             endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request inferences from the endpoint \n",
    "With our model deployed behind a REST API, we'll now make some requests to it in order to get inferences from our validation set. We can then use these inferences to see how well the trained model performs on out-of-sample data.\n",
    "\n",
    "Note that we need to make our request with the payload in `text/csv` format, since that is what our script currently supports (see `input_fn()` in our entrypoint file). If other formats need to be supported, this would have to be added to that `input_fn()` function. Note, however, that we set the `accept` to `application/json` to get our output, i.e. the inferences, that way. We do this because our `ouput_fn()` function returns JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "#read in the validation set from the s3 bucket\n",
    "test_input = f's3://{s3_bucket}/{prefix}/srt_test.csv'\n",
    "\n",
    "#convert it to a pandas dataframe\n",
    "test_df = pd.read_csv(test_input)\n",
    "\n",
    "test_df.columns = ['target', 'text']\n",
    "\n",
    "def format_as_csv(text):\n",
    "    #since we make our request with the payload in text/csv format, we need to sanitize the text first\n",
    "    return text.replace(\",\",\"\").replace(\"\\n\",\"\")\n",
    "\n",
    "test_df['text'] = test_df['text'].apply(format_as_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name,\n",
    "                          sagemaker_session=sagemaker_session,\n",
    "                          serializer=csv_serializer,\n",
    "                          content_type=CONTENT_TYPE_CSV,\n",
    "                          accept=CONTENT_TYPE_JSON)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "samples_to_drop = []\n",
    "for i,v in test_df['text'].iteritems():\n",
    "    try:\n",
    "        pred = predictor.predict(v)\n",
    "        predictions.append(pred)\n",
    "    except Exception as e:\n",
    "        # RealTimePredictor will timeout after 60 seconds and some docs are so large that this occurs\n",
    "        # Since timeouts can be managed in other endpoint deployments and usage patterns,\n",
    "        # we'll pass over this one error for now\n",
    "        print(f\"response for the {i}th sample took too long:\\n{e}\")\n",
    "        samples_to_drop.append(i)\n",
    "        #sleep while server reboots\n",
    "        time.sleep(10)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "y_pred = []\n",
    "for i, p in enumerate(predictions):\n",
    "    result = json.loads(p)\n",
    "    pred = result.get(\"instances\")[0].get('prediction')\n",
    "    y_pred.append(pred)\n",
    "\n",
    "y_true = test_df['target'].tolist()\n",
    "y_true.pop(samples_to_drop[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model performance on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf_report = classification_report(y_true, y_pred)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Endpoint \n",
    "Once we are finished with the endpoint, we clean up the resources since the endpoint incurs costs for as long as it is alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
